{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.framework.ops.Operation'>):\n",
      "<tf.Operation 'init_9' type=NoOp>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"C:\\Users\\a4070\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 344, in do_execute\n",
      "    return reply_content  File \"C:\\Users\\a4070\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)  File \"C:\\Users\\a4070\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2748, in run_cell\n",
      "    return result  File \"C:\\Users\\a4070\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2851, in run_ast_nodes\n",
      "    return True  File \"C:\\Users\\a4070\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2930, in run_code\n",
      "    return outflag  File \"<ipython-input-18-1e573d6473c1>\", line 55, in <module>\n",
      "    sess.run(train)  File \"C:\\Users\\a4070\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 189, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs))\n",
      "==================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.train' has no attribute 'SummaryWriter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-1e573d6473c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mmerged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;31m# writer = tf.summary.FileWriter(\"/tmp/tensorflow_log\", sess.graph_def)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSummaryWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/tmp/tensorflow_log\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.train' has no attribute 'SummaryWriter'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "WW = np.array([[0.1, 0.6, -0.9], \n",
    "               [0.2, 0.5, -0.8], \n",
    "               [0.3, 0.4, -0.7],\n",
    "               [0.4, 0.3, -0.6],\n",
    "               [0.5, 0.2, -0.5]]).astype(np.float32)\n",
    "bb = np.array([0.3, 0.4, 0.5]).astype(np.float32)\n",
    "x_data = np.random.rand(100,5).astype(np.float32)\n",
    "y_data = np.dot(x_data, WW) + bb\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    #mark_used()\n",
    "    W = tf.Variable(tf.random_uniform([5,3], -1.0, 1.0))\n",
    "    # The zeros set to zero with all elements.\n",
    "    b = tf.Variable(tf.zeros([3]))\n",
    "    #y = W * x_data + b\n",
    "    y = tf.matmul(x_data, W) + b\n",
    "\n",
    "    # Add summary ops to collect data\n",
    "    w_hist = tf.summary.scalar(\"weights\", W)\n",
    "    b_hist = tf.summary.scalar(\"biases\", b)\n",
    "    y_hist = tf.summary.scalar(\"y\", y)\n",
    "\n",
    "    # Minimize the mean squared errors.\n",
    "    loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "    # Outputs a Summary protocol buffer with scalar values\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "    # Gradient descent algorithm\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    # Before starting, initialize the variables.  We will 'run' this first.\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    # Creates a SummaryWriter\n",
    "    # Merges all summaries collected in the default graph\n",
    "    merged = tf.summary.merge_all()\n",
    "    # writer = tf.summary.FileWriter(\"/tmp/tensorflow_log\", sess.graph_def)\n",
    "    writer = tf.train.SummaryWriter(\"/tmp/tensorflow_log\", graph=sess.graph)\n",
    "    sess.run(init)\n",
    " \n",
    "    # Fit the line\n",
    "    for step in xrange(501):\n",
    "        if step % 10 == 0:\n",
    "            result = sess.run([merged, loss])\n",
    "            summary_str = result[0]\n",
    "            acc = result[1]\n",
    "            writer.add_summary(summary_str, step)\n",
    "            print(\"step = %s acc = %s W = %s b = %s\" % (step, acc, sess.run(W), sess.run(b)))\n",
    "        else:\n",
    "            sess.run(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load MNIST dataset\n",
      "WARNING:tensorflow:From <ipython-input-4-883665b7da7b>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\a4070\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\a4070\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n"
     ]
    },
    {
     "ename": "ContentTooShortError",
     "evalue": "<urlopen error retrieval incomplete: got only 8388608 out of 9912422 bytes>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mContentTooShortError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-883665b7da7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'load MNIST dataset'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmnist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MNIST_data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mweight_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    304\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m               instructions)\n\u001b[1;32m--> 306\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    308\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py\u001b[0m in \u001b[0;36mread_data_sets\u001b[1;34m(train_dir, fake_data, one_hot, dtype, reshape, validation_size, seed, source_url)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m   local_file = base.maybe_download(TRAIN_IMAGES, train_dir,\n\u001b[1;32m--> 260\u001b[1;33m                                    source_url + TRAIN_IMAGES)\n\u001b[0m\u001b[0;32m    261\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[0mtrain_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    304\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m               instructions)\n\u001b[1;32m--> 306\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    308\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py\u001b[0m in \u001b[0;36mmaybe_download\u001b[1;34m(filename, work_directory, source_url)\u001b[0m\n\u001b[0;32m    250\u001b[0m   \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwork_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m     \u001b[0mtemp_file_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlretrieve_with_retry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m     \u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_file_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    304\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m               instructions)\n\u001b[1;32m--> 306\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    308\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mdelay\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdelays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mis_retriable\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py\u001b[0m in \u001b[0;36murlretrieve_with_retry\u001b[1;34m(url, filename)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0m_internal_retry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_retriable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_is_retriable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0murlretrieve_with_retry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    287\u001b[0m         raise ContentTooShortError(\n\u001b[0;32m    288\u001b[0m             \u001b[1;34m\"retrieval incomplete: got only %i out of %i bytes\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m             % (read, size), result)\n\u001b[0m\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mContentTooShortError\u001b[0m: <urlopen error retrieval incomplete: got only 8388608 out of 9912422 bytes>"
     ]
    }
   ],
   "source": [
    "import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "print('load MNIST dataset')\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "def _write_histogram_summary(parent, infos):\n",
    "    for i in infos:\n",
    "        tf.histogram_summary(\"%s/%s\" % (parent, i[0]), i[1])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    x = tf.placeholder(\"float\", [None, 784])\n",
    "    y_ = tf.placeholder(\"float\", [None, 10])\n",
    "\n",
    "    # 1x28x28 -> 32x28x28 -> 32x14x14\n",
    "    x_image = tf.reshape(x, [-1,28,28,1])    \n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "        b_conv1 = bias_variable([32])\n",
    "        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1, name=scope.name)\n",
    "\n",
    "        _write_histogram_summary('conv1', [['weights', W_conv1],['biases', b_conv1], ['activations', h_conv1]])\n",
    "\n",
    "    h_pool1 = max_pool_2x2(h_conv1)  \n",
    "\n",
    "    # 32x14x14 -> 64x7x7\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "        b_conv2 = bias_variable([64])\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2, name=scope.name)\n",
    "\n",
    "        _write_histogram_summary('conv2', [['weights', W_conv2],['biases', b_conv2], ['activations', h_conv2]])\n",
    "\n",
    "    h_pool2 = max_pool_2x2(h_conv2)    \n",
    "\n",
    "    # 64x7x7 -> 1024\n",
    "    with tf.variable_scope('full1') as scope:\n",
    "        W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "        b_fc1 = bias_variable([1024])\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1, name=scope.name)    \n",
    "\n",
    "        _write_histogram_summary('full1', [['weights', W_fc1],['biases', b_fc1], ['activations', h_fc1]])\n",
    "\n",
    "    # dropout\n",
    "    keep_prob = tf.placeholder(\"float\")\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    # Readout\n",
    "    with tf.variable_scope('full2') as scope:\n",
    "        W_fc2 = weight_variable([1024, 10])\n",
    "        b_fc2 = bias_variable([10])\n",
    "\n",
    "        y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2, name=scope.name)  \n",
    "\n",
    "        _write_histogram_summary('full2', [['weights', W_fc2],['biases', b_fc2]])\n",
    "\n",
    "    tf.histogram_summary(\"y\", y_conv)\n",
    "\n",
    "    cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "\n",
    "    tf.scalar_summary(\"cross_entropy\", cross_entropy)\n",
    "    tf.scalar_summary(\"accuracy\", accuracy)\n",
    "\n",
    "    # Before starting, initialize the variables.  We will 'run' this first.\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    # Creates a SummaryWriter\n",
    "    # Merges all summaries collected in the default graph\n",
    "    merged = tf.merge_all_summaries()\n",
    "    writer = tf.train.SummaryWriter(\"/tmp/tensorflow_log_mnist\", sess.graph_def)\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    # training\n",
    "    N = len(mnist.train.images)\n",
    "    N_test = len(mnist.test.images)\n",
    "    n_epoch = 20000\n",
    "    batchsize = 50\n",
    "\n",
    "    for i in range(n_epoch):\n",
    "        batch = mnist.train.next_batch(batchsize)\n",
    "        if i%100 == 0:\n",
    "            summary_str, loss_value = sess.run([merged, accuracy], feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "            writer.add_summary(summary_str, i)\n",
    "            print(\"step %d %.2f\" % (i, loss_value))\n",
    "        sess.run(train_step, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})    \n",
    "\n",
    "    tacc = 0\n",
    "    tbatchsize = 1000\n",
    "    for i in range(0,N_test,tbatchsize):\n",
    "        acc = sess.run(accuracy, feed_dict={\n",
    "        x: mnist.test.images[i:i+tbatchsize], y_: mnist.test.labels[i:i+tbatchsize], keep_prob: 1.0})\n",
    "        tacc += acc * tbatchsize\n",
    "\n",
    "        print(\"test step %d acc = %.2f\" % (i//tbatchsize, acc))\n",
    "    tacc /= N_test\n",
    "    print(\"test accuracy %.2f\" % tacc)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
